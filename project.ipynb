{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "82744cc1-1f98-48c4-fc6b-49631b89afdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INITIAL SAMPLE 1984\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "83ddd35e-a87d-40ee-bd4c-f82d6dbbd4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 7 ACTION -1 NEXT REWARD 0 NEXT STATE 134\n",
            "CURR STATE 134 ACTION 1 NEXT REWARD 0 NEXT STATE 135\n",
            "CURR STATE 135 ACTION -1 NEXT REWARD 0 NEXT STATE 262\n",
            "CURR STATE 262 ACTION 1 NEXT REWARD 0 NEXT STATE 263\n",
            "CURR STATE 263 ACTION 1 NEXT REWARD 0 NEXT STATE 384\n",
            "CURR STATE 384 ACTION 0 NEXT REWARD 0 NEXT STATE 512\n",
            "CURR STATE 512 ACTION 0 NEXT REWARD 0 NEXT STATE 640\n",
            "CURR STATE 640 ACTION 0 NEXT REWARD 0 NEXT STATE 768\n",
            "CURR STATE 768 ACTION 0 NEXT REWARD 0 NEXT STATE 896\n",
            "CURR STATE 896 ACTION 0 NEXT REWARD 0 NEXT STATE 1024\n",
            "CURR STATE 1024 ACTION 1 NEXT REWARD 0 NEXT STATE 1153\n",
            "CURR STATE 1153 ACTION 1 NEXT REWARD 0 NEXT STATE 1290\n",
            "CURR STATE 1290 ACTION 0 NEXT REWARD 0 NEXT STATE 1298\n",
            "CURR STATE 1298 ACTION 1 NEXT REWARD 0 NEXT STATE 1307\n",
            "CURR STATE 1307 ACTION 1 NEXT REWARD 0 NEXT STATE 1188\n",
            "CURR STATE 1188 ACTION 1 NEXT REWARD 0 NEXT STATE 1061\n",
            "CURR STATE 1061 ACTION -1 NEXT REWARD 0 NEXT STATE 924\n",
            "CURR STATE 924 ACTION 0 NEXT REWARD 0 NEXT STATE 796\n",
            "CURR STATE 796 ACTION 0 NEXT REWARD 0 NEXT STATE 668\n",
            "CURR STATE 668 ACTION -1 NEXT REWARD 0 NEXT STATE 539\n",
            "CURR STATE 539 ACTION 0 NEXT REWARD 0 NEXT STATE 419\n",
            "CURR STATE 419 ACTION -1 NEXT REWARD 0 NEXT STATE 298\n",
            "CURR STATE 298 ACTION -1 NEXT REWARD 0 NEXT STATE 305\n",
            "CURR STATE 305 ACTION 1 NEXT REWARD 0 NEXT STATE 442\n",
            "CURR STATE 442 ACTION 0 NEXT REWARD 0 NEXT STATE 450\n",
            "CURR STATE 450 ACTION -1 NEXT REWARD 0 NEXT STATE 457\n",
            "CURR STATE 457 ACTION 0 NEXT REWARD 0 NEXT STATE 593\n",
            "CURR STATE 593 ACTION 0 NEXT REWARD 0 NEXT STATE 729\n",
            "CURR STATE 729 ACTION -1 NEXT REWARD 0 NEXT STATE 864\n",
            "CURR STATE 864 ACTION -1 NEXT REWARD 0 NEXT STATE 999\n",
            "CURR STATE 999 ACTION 0 NEXT REWARD 0 NEXT STATE 1119\n",
            "CURR STATE 1119 ACTION 1 NEXT REWARD 0 NEXT STATE 1232\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "with open(\"log-random.tsv\", \"w\") as f:\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\n\")\n",
        "\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "        print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "        f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "Q = {}\n",
        "\n",
        "def set_state(Q, state, actions):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in actions}\n",
        "\n",
        "for episode in range(32):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "        #print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "        #pass\n",
        "        curr_actions = simulator.get_actions(curr_state)\n",
        "        next_actions = simulator.get_actions(next_state)\n",
        "        set_state(Q, curr_state, curr_actions)\n",
        "        set_state(Q, next_state, next_actions)\n",
        "\n",
        "        if next_state == simulator.terminal_state:\n",
        "            max_next = 0.0\n",
        "        else:\n",
        "            max_next = max(Q[next_state][a] for a in next_actions)\n",
        "\n",
        "        # Q-learning\n",
        "        target = next_reward + gamma * max_next\n",
        "        Q[curr_state][curr_action] = target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W8cFRd7uPGqy"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "\n",
        "Q = {}\n",
        "\n",
        "def set_state(Q, state, actions):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in actions}\n",
        "\n",
        "with open(\"q-random.tsv\", \"w\") as f:\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\told_value\\tnew_value\\n\")\n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(\n",
        "            random_policy, max_steps=1024\n",
        "        ):\n",
        "            # Ensure both states in table\n",
        "            curr_actions = simulator.get_actions(curr_state)\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            set_state(Q, curr_state, curr_actions)\n",
        "            set_state(Q, next_state, next_actions)\n",
        "\n",
        "            # Old value before update\n",
        "            old_value = Q[curr_state][curr_action]\n",
        "\n",
        "            # Compute target: r + gamma * max_a' Q(s',a')  (0 if terminal sink)\n",
        "            if next_state == simulator.terminal_state:\n",
        "                max_next = 0.0\n",
        "            else:\n",
        "                max_next = max(Q[next_state][a] for a in next_actions)\n",
        "            target = next_reward + gamma * max_next\n",
        "\n",
        "            # Q-learning update (alpha = 1 -> direct assignment)\n",
        "            new_value = (1 - alpha) * old_value + alpha * target\n",
        "            Q[curr_state][curr_action] = new_value\n",
        "\n",
        "            f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\t{old_value}\\t{new_value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pS7g1sETAbKd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "def set_state(Q, state, actions):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in actions}\n",
        "    else:\n",
        "        # Ensure all currently valid actions exist\n",
        "        for a in actions:\n",
        "            if a not in Q[state]:\n",
        "                Q[state][a] = 0.0\n",
        "\n",
        "def greedy_action(Q, state, actions):\n",
        "    set_state(Q, state, actions)\n",
        "    #best_a = actions[0]\n",
        "    #best_v = Q[state][best_a]\n",
        "    #for a in actions[1:]:\n",
        "     #   v = Q[state][a]\n",
        "      #  if v > best_v:\n",
        "       #     best_a, best_v = a, v\n",
        "    #return best_a\n",
        "    return max(actions, key=lambda a: Q[state][a])\n",
        "\n",
        "# hard-code epsilon=0.25. this is high but the environment is deterministic.\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    epsilon = 0.25\n",
        "    set_state(Q, state, actions)\n",
        "    # 25% randomness\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    return greedy_action(Q, state, actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JcNQg6qRAsqc"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "Q = {}\n",
        "\n",
        "# Build episode start sequence that GUARANTEES coverage of 0 and 1, then cycles all initial states\n",
        "init_list = list(simulator.initial_states)\n",
        "ordered_starts = []\n",
        "# put 0 and 1 first if they are valid initial states\n",
        "for must in (0, 1):\n",
        "    if must in init_list and must not in ordered_starts:\n",
        "        ordered_starts.append(must)\n",
        "# then append the rest (without duplicates)\n",
        "for s in init_list:\n",
        "    if s not in ordered_starts:\n",
        "        ordered_starts.append(s)\n",
        "\n",
        "episodes = 32\n",
        "steps_per_episode = 1024\n",
        "\n",
        "observed_curr_states = set()  # track exactly which states appear as curr_state\n",
        "\n",
        "with open(\"q-greedy.tsv\", \"w\") as f:\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\told_value\\tnew_value\\n\")\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        # cycle through ordered starts so that 0 and 1 are guaranteed to appear\n",
        "        start_state = ordered_starts[ep % len(ordered_starts)]\n",
        "        curr_state = start_state\n",
        "\n",
        "        for t in range(steps_per_episode):\n",
        "            observed_curr_states.add(curr_state)\n",
        "\n",
        "            actions = simulator.get_actions(curr_state)\n",
        "            curr_action = epsilon_greedy_policy(curr_state, actions)\n",
        "\n",
        "            next_reward, next_state = simulator.get_next_reward_state(curr_state, curr_action)\n",
        "\n",
        "            # bootstrap\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            set_state(Q, curr_state, actions)\n",
        "            set_state(Q, next_state, next_actions)\n",
        "\n",
        "            old_value = Q[curr_state][curr_action]\n",
        "            if next_state == simulator.terminal_state:\n",
        "                max_next = 0.0\n",
        "            else:\n",
        "                max_next = max(Q[next_state][a] for a in next_actions)\n",
        "            target = next_reward + gamma * max_next\n",
        "            new_value = target  # alpha = 1.0\n",
        "\n",
        "            Q[curr_state][curr_action] = new_value\n",
        "\n",
        "            # log\n",
        "            f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\t{old_value}\\t{new_value}\\n\")\n",
        "\n",
        "            # advance (ensures sequential continuity within the episode)\n",
        "            curr_state = next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote policy-greedy.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "epsilon = 0.25\n",
        "Q = {}  # final Q(s,a) reconstructed from q-greedy.tsv\n",
        "\n",
        "# Load final Q(s,a) from q-greedy.tsv (keep the last new_value per (s,a))\n",
        "with open(\"q-greedy.tsv\", \"r\") as f:\n",
        "    header = f.readline().rstrip(\"\\n\").split(\"\\t\")\n",
        "    i_s = header.index(\"curr_state\")\n",
        "    i_a = header.index(\"curr_action\")\n",
        "    i_v = header.index(\"new_value\")\n",
        "\n",
        "    for line in f:\n",
        "        parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "        if len(parts) <= i_v:\n",
        "            continue\n",
        "        s = int(parts[i_s])\n",
        "        a = int(parts[i_a])\n",
        "        v = float(parts[i_v])\n",
        "        if s not in Q:\n",
        "            Q[s] = {}\n",
        "        # Overwrite so the last occurrence becomes the final learned Q(s,a)\n",
        "        Q[s][a] = v\n",
        "\n",
        "def best_action_for_state_from_Q(s):\n",
        "    if s in Q and Q[s]:\n",
        "        best_a = None\n",
        "        best_v = None\n",
        "        for a, v in Q[s].items():\n",
        "            if best_v is None or v > best_v:\n",
        "                best_a, best_v = a, v\n",
        "        return best_a\n",
        "    actions = simulator.get_actions(s)\n",
        "    return actions[0] if actions else 0\n",
        "\n",
        "# Write the greedy policy only for states observed as curr_state\n",
        "with open(\"policy-greedy.tsv\", \"w\") as f:\n",
        "    f.write(\"state\\taction\\n\")\n",
        "    for s in sorted(observed_curr_states):\n",
        "        a = best_action_for_state_from_Q(s)\n",
        "        f.write(f\"{s}\\t{a}\\n\")\n",
        "\n",
        "print(\"Wrote policy-greedy.tsv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b1A9W4gCDiRZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "alpha = 1.0        # deterministic env -> direct target\n",
        "gamma = 0.9\n",
        "epochs = 20        # passes over all initial states\n",
        "max_steps = 2048   # safety cap per episode\n",
        "eps_start = 0.30   # more exploration early\n",
        "eps_end   = 0.01   # small exploration late\n",
        "Q_opt = {}\n",
        "\n",
        "def greedy_action(Q, state, actions):\n",
        "    set_state(Q, state, actions)\n",
        "    best_a = actions[0]\n",
        "    best_v = Q[state][best_a]\n",
        "    for a in actions[1:]:\n",
        "        v = Q[state][a]\n",
        "        if v > best_v:\n",
        "            best_a, best_v = a, v\n",
        "    return best_a\n",
        "\n",
        "def epsilon_greedy(Q, state, actions, epsilon):\n",
        "    set_state(Q, state, actions)\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    return greedy_action(Q, state, actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_states = list(simulator.initial_states)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Linear epsilon decay across epochs\n",
        "    epsilon = eps_end + (eps_start - eps_end) * max(0.0, (epochs - 1 - epoch) / max(1, (epochs - 1)))\n",
        "    random.shuffle(init_states)  # different order each pass\n",
        "\n",
        "    for s0 in init_states:\n",
        "        state = s0\n",
        "        for t in range(max_steps):\n",
        "            actions = simulator.get_actions(state)\n",
        "            action = epsilon_greedy(Q_opt, state, actions, epsilon)\n",
        "\n",
        "            reward, next_state = simulator.get_next_reward_state(state, action)\n",
        "\n",
        "            # Prepare next state's action set for bootstrap\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            set_state(Q_opt, next_state, next_actions)\n",
        "\n",
        "            # Q-learning target and update\n",
        "            if next_state == simulator.terminal_state:\n",
        "                max_next = 0.0\n",
        "            else:\n",
        "                max_next = max(Q_opt[next_state][a] for a in next_actions)\n",
        "\n",
        "            target = reward + gamma * max_next\n",
        "            Q_opt[state][action] = target\n",
        "\n",
        "            # Move forward\n",
        "            state = next_state\n",
        "\n",
        "            # cut off after first non-zero reward, but only after update\n",
        "            if reward > 0:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"policy-optimal.tsv\", \"w\") as f:\n",
        "    f.write(\"state\\taction\\n\")\n",
        "    for y in range(simulator.resolution):\n",
        "        for x in range(simulator.resolution):\n",
        "            for direction in range(8):\n",
        "                s = simulator.construct_state(x, y, direction)\n",
        "                actions = simulator.get_actions(s)\n",
        "                set_state(Q_opt, s, actions)\n",
        "                a_star = greedy_action(Q_opt, s, actions)\n",
        "                f.write(f\"{s}\\t{a_star}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
